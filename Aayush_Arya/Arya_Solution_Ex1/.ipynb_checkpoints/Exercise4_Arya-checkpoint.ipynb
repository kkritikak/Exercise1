{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total(17.5/20)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['text.usetex'] = True\n",
    "#plt.rcParams['font.family'] = 'serif'\n",
    "#plt.rcParams['font.serif'] = ['Computer Modern Serif']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1    (4.5/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_nums = np.random.uniform(size=100_000)  #1 point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('generated_nums.txt', random_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the numbers saved in `generated_nums.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_nums = np.loadtxt('generated_nums.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average(nums: np.ndarray):\n",
    "    \"\"\"\n",
    "    Takes an np.ndarray object and calculates the average of the entries in that array\n",
    "\n",
    "    Note: indicating the variable type to np.ndarray in the function arguments does not\n",
    "    put a requirement, but rather only serves as guidance (shows up in generated docs)\n",
    "\n",
    "    \"\"\"\n",
    "    return nums.sum()/len(nums)\n",
    "\n",
    "\n",
    "def compute_variance(nums: np.ndarray):\n",
    "    \"\"\"\n",
    "    Computes sigma^2\n",
    "    \"\"\"\n",
    "    avg_of_squares = (nums**2).sum()/len(nums)\n",
    "    square_of_avg = (compute_average(nums)**2)/len(nums)\n",
    "\n",
    "    return avg_of_squares-square_of_avg\n",
    "\n",
    "\n",
    "def compute_err_mean(nums):\n",
    "    err = np.sqrt(compute_variance(nums)/len(nums))\n",
    "    return err\n",
    "#0.5 points\n",
    "#mistake--\n",
    "#the formula for variance is <x^2>-<x^2>, but while while calculating square_of_avg you mistakenly\n",
    "#divided <x>^2 one more time by len(nums) but len(nums) was already included in your compute_average function\n",
    "#so it is giving wrong value of variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The mean of the random numbers is', compute_average(saved_nums))\n",
    "print('Variance:', compute_variance(saved_nums))\n",
    "print('Error of the mean value', compute_err_mean(saved_nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs, bin_edges = np.histogram(saved_nums, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x=[(bin_edges[i] + bin_edges[i-1])/2 for i in range(1, len(bin_edges))],\n",
    "                 bins=bin_edges, weights=freqs, ec='k', lw=0.5)\n",
    "plt.axhline(y=1000, xmin=0, xmax=1, ls='--', lw=0.5, c='r')\n",
    "plt.xlabel('Value [0-1]')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency distribution of the generated numbers') \n",
    "\n",
    "#1point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rands, y_rands = np.random.uniform(size=(2,100_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_centres(bin_edges):\n",
    "    return [(bin_edges[i] + bin_edges[i-1])/2 for i in range(1, len(bin_edges))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(7,6), width_ratios=[6,1])\n",
    "hist_2d = axes[0].hist2d(x_rands, y_rands, bins=50, cmap='magma')\n",
    "cbar = plt.colorbar(hist_2d[3], cax=axes[1], label='Frequency')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "plt.suptitle('Frequency distribution of each (x,y) pair')\n",
    "\n",
    "#1point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearCongruentialNumGen():\n",
    "    \"\"\"\n",
    "    A class for creating an instance of our own generator\n",
    "    \"\"\"\n",
    "\n",
    "    a = 1664525 # suggested parameters\n",
    "    c = 1013904223\n",
    "    modulus = 2**32\n",
    "\n",
    "    global last_gen\n",
    "    last_gen = None\n",
    "\n",
    "\n",
    "    def __init__(self, seed=0.5):\n",
    "        self.seed = seed\n",
    "        global last_gen\n",
    "        if last_gen == None:\n",
    "            last_gen = seed\n",
    "\n",
    "\n",
    "    def hard_reset(self):\n",
    "        last_gen = self.seed\n",
    "\n",
    "\n",
    "    def next_num(self) -> float:\n",
    "        \"\"\"\n",
    "        This is the function that generates each x_{n+1}\n",
    "        \"\"\"\n",
    "        global last_gen\n",
    "        last_gen = ((self.a*last_gen + self.c) % self.modulus)\n",
    "        return last_gen/self.modulus\n",
    "\n",
    "\n",
    "    def random(self, N: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate N (pseudo)random numbers and return them as an np.ndarray\n",
    "        \"\"\"\n",
    "        return np.array([self.next_num() for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gen = LinearCongruentialNumGen(seed=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gen.random(10)\n",
    "#1point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (5/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_pi(grid_length = 100000):\n",
    "    radius = 1\n",
    "    x, y = np.random.uniform(size=(2, grid_length))\n",
    "\n",
    "    inside = sum(np.sqrt(x**2 + y**2) <= radius)\n",
    "    frac_inside = inside/grid_length\n",
    "\n",
    "    pi = frac_inside*4\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_est = approx_pi(1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "true_val = 3.1415926535897932384626433830795 # written down from memory, I can write more ;)\n",
    "print('Accuracy:', (1-abs(pi_est-true_val))*100, '%')\n",
    "#5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is after drawing a sample of 1 million numbers? My memory can do so much better ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3( 4 / 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a) (1/1)\n",
    "The metropolis algorithm enforces detailed balance. This can be seen as follows:\n",
    "\n",
    "__Proof__: In the canonical ensemble, probability of existing in state $i$ is given by the Boltzmann factor $e^{-\\beta E_i}$.\n",
    "\n",
    "Consider states $i$ and $j$, such that $\\omega_{ij} = P_j/P_i < 1$. This means $e^{-\\beta(E_j-E_i)} > 0$, or simply $\\Delta E > 0$.\n",
    "\n",
    "According to the metropolis criterion, $$\\omega_{ji} = min\\left(1, \\frac{P_i}{P_j} = e^{\\beta\\Delta E}\\right)$$\n",
    "and since for $\\Delta E > 0$, this in turn gives $\\omega_{ji} = 1$.\n",
    "\n",
    "Then, we get, $$ \\frac{\\omega_{ij}}{\\omega_{ji}} = \\frac{P_j/P_i}{1}$$\n",
    "which in turn implies\n",
    "$\\implies P_i \\omega_{ij} = P_j \\omega_{ji}$, which is essentially detailed balance.\n",
    "\n",
    "Showing that this holds even when $P_j/P_i > 1,\\ \\Delta E < 0$ is trivial. Since $i$ and $j$ are independent indices that run over the same range, swapping the indices has no effect. Therefore, it holds also for the converse case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b): Heat bath algorithm(2/2)\n",
    "If the transition probability is given as $$\\omega_{ij} = \\frac{e^{-\\beta \\Delta E}}{e^{-\\beta\\Delta E} + 1} $$ where $\\Delta E = E_j - E_i$. Then the probability for the other transition $$ \\omega_{ji} = \\frac{e^{\\beta\\Delta E}}{e^{\\beta\\Delta E} + 1} $$\n",
    "\n",
    "The ratio of the transition probabilities, then $$ \\frac{\\omega_{ij}}{\\omega_{ji}} = \\frac{e^{-\\beta\\Delta E} (e^{\\beta\\Delta E} + 1 )}{(e^{-\\beta \\Delta E} + 1)e^{\\beta\\Delta E}} = \\frac{1 + e^{-\\beta\\Delta E}}{1 + e^{\\beta\\Delta E}}$$\n",
    "\n",
    "Given the Boltzmann probability of state $i$ occuring, the ratio $$ \\frac{P_j}{P_i} = e^{-\\beta\\Delta E}$$ holds due to the nature of the ensemble (canonical).\n",
    "\n",
    "Then, substituting this in the expression above\n",
    "\n",
    "$$\\frac{\\omega_{ij}}{\\omega_{ji}} = \\frac{1 + P_j/P_i}{1 + P_i/P_j} = \\frac{(P_i + P_j)/P_i}{(P_j + P_i)/P_j}  $$\n",
    "\n",
    "$$ \\implies \\boxed{\\frac{\\omega_{ij}}{\\omega_{ji}} = \\frac{P_j}{P_i} = e^{-\\beta\\Delta E}}$$\n",
    "\n",
    "which is the desired result. Therefore, the heat bath algorithm also ensures detailed balance for the canonical ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (c)(1/1)\n",
    "the idea of solving the question is somewhere correct , V was provided in the question , since in $\\Delta E \\to 0$ ; \n",
    "${e^{-\\beta \\Delta E}}\\to 1$ and in $\\Delta E \\to \\infty$ ;  ${e^{-\\beta \\Delta E}}\\to 0$$\\\\$\n",
    "So $\\omega_{ij} = \\frac{e^{-\\beta\\Delta E}}{e^{-\\beta\\Delta E} + 1} \\to$ 1/2 ; 0 in $r_{ij}$ different cases.\n",
    "\n",
    "$\\rightarrow$ I'm sorry, but.. what does the question mean, exactly? How many particles do we assume? What's the dimensions of the 2D box we are considering? I only understand the interaction range is $\\sim \\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a system of two \"hard\" discs in a plane of finite dimensions $a\\times a$. There are two possible macrostates, with $E = 0$ and $E=\\infty$. Let's call the former state $i$ and latter state $j$.\n",
    "\n",
    "The Boltzmann probability $P_i = e^{-\\beta\\Delta E_i} = e^{0} = 1$ and\n",
    "\n",
    " $P_j = e^{-\\beta\\Delta E_j} = e^{-\\infty} = 0$\n",
    "\n",
    " __Metropolis__\n",
    " Since $P_i/P_j \\to \\infty$, the transition $j\\to i$ should have the probability $\\text{min}(1, \\infty) = 1$.\n",
    " Conversely, $P_j/P_i = 0$, and so $\\omega_{ij} = 0$.\n",
    "\n",
    " This satisfies detailed balance trivially, $P_j \\omega_{ji} = P_i \\omega_{ij} = 0$\n",
    "\n",
    " __Heat Bath__\n",
    " Note that since in this algorithm $$\\omega_{ij} = \\frac{e^{-\\beta\\Delta E}}{e^{-\\beta\\Delta E} + 1}$$\n",
    " Each of the $\\omega$'s are typically bounded from above due to the nature of the expression (at max equal to 1). The two probabilities are still in the $\\Delta E \\to 0$ and asymptotic $\\Delta E \\to \\infty$ limits\n",
    "\n",
    " $$\\omega_{ji} = \\frac{e^{\\beta\\Delta E}}{1 + e^{\\beta \\Delta E}} = \\lim_{\\Delta E \\to \\infty}\\left(1 - \\frac{1}{e^{\\beta\\Delta E} + 1}\\right) \\simeq 1$$\n",
    " and $\\omega_{ij} = 0$. Once again, detailed balance is satisfied trivially $P_j \\omega_{ji} = P_i \\omega_{ij} = 0$.\n",
    "\n",
    " The hard disks problem is thus one case in which the $\\omega$'s given by either algorithms are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4(4/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)(2.5/3) \n",
    "At low temperatures, the system retains its configuration. In the presence of a field $H$, all spins will remain aligned or anti-aligned. In absence of any field, the system keeps its magnetisation.\n",
    "\n",
    "As the temperature is raised, spontaneous flipping of certain spins occurs.\n",
    "\n",
    "As the temperature is continually raised, a phase transition occurs from a state of aligned spins to completely random spins. The transition occurs smoothly between $1.5 < T < 3.5$ and peaks _roughly_ around 2.4(?). This is a second-order phase transition.\n",
    "\n",
    "The Wolff algorithm is suited for higher temperatures. At lower temperatures, the size of the clusters is too large and thus the algorithm sometimes flips the entire grid. In those cases, Metropolis would provide a more physical update.\n",
    "\n",
    "However, Metropolis is slower at higher temperatures, since every single particle in the grid is updated individually.\n",
    "\n",
    "\n",
    "#### Mistake:\n",
    "single flip will go fo high temperatures and the wolff will go for low temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)(1.5/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm for single spin-flip\n",
    "* Choose a random spin to flip\n",
    "* Draw a random $r \\in (0, 1)$\n",
    "* _iff_ $r \\leq e^{-\\beta \\Delta E}$, flip the spin of this particle.\n",
    "    * if $\\Delta E < 0$, then $e^{-\\beta \\Delta E} > 1$ and this is always satisfied. So, for negative $\\Delta E$, accept the flip move trivially.\n",
    "    \n",
    "#### 1.5 deducted for not answering in part (b)?  \n",
    ":Which different values for the energy difference Δ𝐸 occur in 1D and 2D, respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
